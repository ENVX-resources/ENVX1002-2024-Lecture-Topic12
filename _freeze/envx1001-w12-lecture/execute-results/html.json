{
  "hash": "bc8951203678febf5feb1c45183c3f09",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Quarto reveal.js Template\nsubtitle: School of Life and Environmental Sciences (SOLES)\nauthor: Januar Harianto\ninstitute: The University of Sydney\ndate: last-modified # today | last-modified\ndate-format: \"MMM YYYY\" # see https://momentjs.com/docs/#/displaying/format/\nexecute: \n  eval: true\n  echo: true\nself-contained: false\n---\n\n\n\n\n# Recap\nLast lecture...\n\n## Regressions \n\n### Simple linear regression\n\n$$ Y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i $$\n\nIdeal for predicting a continuous response variable from a single predictor variable: *\"How does $y$ change as $x$ changes, when the relationship is linear?\"*\n\n\n### Multiple linear regression \n\n$$ Y_i = \\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i} + ... + \\beta_k x_{ki} + \\epsilon_i $$\n\n*\"How does $y$ change as $x_1$, $x_2$, ..., $x_k$ change?\"*\n\n\n:::{.fragment}\n### Nonlinear regression\n$$ Y_i = f(x_i, \\beta) + \\epsilon_i $$\n:::\n\n:::{.fragment}\nwhere $f(x_i, \\beta)$ is a nonlinear function of the parameters $\\beta$: \"How do we model a change in $y$ with $x$ when the relationship is nonlinear?\"\n:::\n\n# Nonlinear regression\n\n![](images/gauss.jpg)\n![](images/netwon.jpg)\n\n*Carl Friedrich Gauss (1777-1855) and Isaac Newton (1642-1726).*\n\n\n## Fitting a nonlinear model\n\nLinear relationships are simple to interpret since the rate of change is constant.\n\n> \"As one changes, the other changes at a constant rate.\"\n\nNonlinear relationships often involve exponential, logarithmic, or power functions.\n\n> \"As one changes, the other changes at a rate that is *not proportional* to the change in the other.\"\n\n\n## Nonlinear relationships: exponents\n\n- $x^2$ is the *square* of $x$.\n- $x^3$ is the *cube* of $x$.\n- $x^a$ is x raised to the *power* of $a$.\n\n> In a relationship where $y$ is a function of $x^a$, as $y$ increases, $x$ increases at a rate that is equal to $x$ to the power of $a$.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n# Plot a simulation of above in ggplot2\nset.seed(123)\ntibble(x = seq(0, 10, by = 0.2), y = x^2) %>%\n  ggplot(aes(x = x, y = y)) +\n  geom_point() +\n  labs(x = \"x\", y = \"y\") +\n  ggtitle(expression(y == x^2)) +\n  theme(plot.title = element_text(size = 40, face = \"bold\"))\n```\n\n::: {.cell-output-display}\n![](envx1001-w12-lecture_files/figure-revealjs/unnamed-chunk-1-1.png){width=960}\n:::\n:::\n\n\n## Nonlinear relationships: logarithms\n\n- $log_e(x)$ is the *natural logarithm* of $x$.\n- $log_{10}(x)$ is the *common logarithm* of $x$.\n- $log_a(x)$ is the *logarithm* of $x$ to the base $a$.\n\n**Interpretation:**\n\n- If $\\log_a(y) = x$: as $x$ increases, $y$ increases at a rate of $y = a^x$.\n- If $y = \\log_a(x)$: as $y$ increases, $x$ also increases, at $x = a^y$.\n\n\n## Exponents and logarithms\n\n\n|       | Exponents        | Logarithms       |\n|-------|:----------------:|:----------------:|\n| **Definition** | If $a^n = b$, $a$ is the base, $n$ is the exponent, and $b$ is the result. | If $\\log_a b = n$, $a$ is the base, $b$ is the result, and $n$ is the logarithm (or the exponent in the equivalent exponential form). |\n| **Example** | $2^3 = 8$ | $\\log_2 8 = 3$ |\n| **Interpretation** | $2$ raised to the power of $3$ equals $8$. | The power to which you must raise $2$ to get $8$ is $3$. |\n| **Inverse** | The logarithm is the inverse operation of exponentiation. | The exponentiation is the inverse operation of logarithm. |\n| **Properties** | $(a^n)^m = a^{n \\cdot m}$, $a^n \\cdot a^m = a^{n+m}$, $\\frac{a^n}{a^m} = a^{n-m}$ | $\\log_a(b \\cdot c) = \\log_a b + \\log_a c$, $\\log_a\\left(\\frac{b}{c}\\right) = \\log_a b - \\log_a c$, $\\log_a(b^n) = n \\cdot \\log_a b$ |\n\n\n## Dealing with nonlinearity\n\n\n### Transformations\n\nOften, a nonlinear relationship may be transformed into a linear relationship by applying a transformation to the response variable or the predictor variable(s).\n\n- **Logarithmic**: $y = \\log(x)$ \n- **Exponential**: $y = e^x$\n- **Square-root**: $y = \\sqrt{x}$\n- **Inverse**: $y = \\frac{1}{x}$\n\n:::{.fragment}\n- All good when $y$ changes [monotically](https://en.wikipedia.org/wiki/Monotonic_function) with $x$.\n- What if relationship is not monotonic, or is more complex?\n:::\n\n# Common nonlinear functions\n\n$f(x_i, \\beta)$\n\n## Exponential decay relationship\n\nResponse variable *decreases* and approaches limit as predictor variable increases.\n\n$$ y = a \\cdot e^{-b_x} $$\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nset.seed(429) # set seed\n# Simulate data:\ndecay <- tibble(\n  predictor = seq(0,10, by = 0.2),\n  response = abs(exp(-0.5*predictor) + rnorm(length(predictor), mean = 1, sd = 0.1)))\n\nggplot(data = decay, aes(x = predictor, y = response)) +\n  geom_point() +\n  labs(x = \"Predictor\", y = \"Response\")\n```\n\n::: {.cell-output-display}\n![](envx1001-w12-lecture_files/figure-revealjs/unnamed-chunk-2-1.png){width=960}\n:::\n:::\n\n\nExamples: radioactive decay, population decline, chemical reactions.\n\n## Asymptotic relationship\n\nResponse variable *increases* and approaches a limit as the predictor variable increases.\n\n$$ y = a + b(1 - e^{-cx}) $$\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nset.seed(442) # set seed\n# Simulate data:\nasymptotic = tibble(\n  predictor = seq(0, 10, by = 0.2),\n  response = 100*(1-exp(-0.5*predictor)) + rnorm(length(predictor), mean = 0, sd = 10))\n\nggplot(data = asymptotic, aes(x = predictor, y = response)) +\n  geom_point() +\n  labs(x = \"Predictor\", y = \"Response\")\n```\n\n::: {.cell-output-display}\n![](envx1001-w12-lecture_files/figure-revealjs/unnamed-chunk-3-1.png){width=960}\n:::\n:::\n\n\nExamples: population growth, enzyme kinetics.\n\n## Logistic relationship\n\nAn S-shaped relationship, where the response variable is at first exponential, then asymptotic.\n\n$$ y = c + \\frac{d-c}{1+e^{-b(x-a)}} $$\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nset.seed(450)\n# Simulate data:\nlogistic <- tibble(predictor = seq(0, 10, by = 0.2), \n  response = 10 + abs(300 * (1 / (1 + exp(-0.8 * (predictor - 5)))) + rnorm(length(predictor), mean = 0, sd = 10)))\n\nggplot(data = logistic, aes(x = predictor, y = response)) +\n  geom_point() +\n  labs(x = \"Predictor\", y = \"Response\")\n```\n\n::: {.cell-output-display}\n![](envx1001-w12-lecture_files/figure-revealjs/unnamed-chunk-4-1.png){width=960}\n:::\n:::\n\n\nExamples: growth of bacteria, disease spread, species growth.\n\n## Curvilinear relationship\n\nResponse variable changes in a variety of ways as the predictor variable changes.\n\n$$ y = a + bx + cx^2 + dx^3 + ... $$\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n# Set seed for reproducibility\nset.seed(529)\n# Simulate data:\ncurvilinear <- tibble(predictor = seq(0, 30, length.out = 50), \n  response = 50 * (1 - (predictor - 15)^2 / 225) + rnorm(length(predictor), mean = 0, sd = 5))\n\nggplot(data = curvilinear, aes(x = predictor, y = response)) +\n  geom_point() +\n  labs(x = \"Predictor\", y = \"Response\")\n```\n\n::: {.cell-output-display}\n![](envx1001-w12-lecture_files/figure-revealjs/unnamed-chunk-5-1.png){width=960}\n:::\n:::\n\n\nExamples: food intake, drug dosage, exercise.\n\n# Transformations\n> How far can we go?\n\n## Transformations: Exponential decay\n\n::::{.columns}\n:::{.column width=\"50%\"}\n#### Before transformation\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(data = decay,\n       aes(x = predictor, y = response)) +\n  geom_point() +\n  labs(x = \"Predictor\", y = \"Response\")\n```\n\n::: {.cell-output-display}\n![](envx1001-w12-lecture_files/figure-revealjs/unnamed-chunk-6-1.png){width=960}\n:::\n:::\n\n:::\n\n:::{.column width=\"50%\"}\n:::{.fragment}\n#### After log~e~ transform\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(data = decay, \n       aes(x = predictor, y = log(response))) +\n  geom_point() +\n  labs(x = \"Predictor\", y = \"Response\")\n```\n\n::: {.cell-output-display}\n![](envx1001-w12-lecture_files/figure-revealjs/unnamed-chunk-7-1.png){width=960}\n:::\n:::\n\n:::\n:::\n::::\n\n## Transformations: Exponential decay\n\n::::{.columns}\n:::{.column width=\"50%\"}\n#### Before transformation\n\n::: {.cell}\n\n```{.r .cell-code}\nautoplot(lm(response ~ predictor, data = decay)) +\n  labs(x = \"Predictor\", y = \"Response\")\n```\n\n::: {.cell-output-display}\n![](envx1001-w12-lecture_files/figure-revealjs/unnamed-chunk-8-1.png){width=960}\n:::\n:::\n\n:::\n:::{.column width=\"50%\"}\n#### After log~e~ transform\n\n::: {.cell}\n\n```{.r .cell-code}\nautoplot(lm(log(response) ~ predictor, data = decay)) +\n  labs(x = \"Predictor\", y = \"Response\")\n```\n\n::: {.cell-output-display}\n![](envx1001-w12-lecture_files/figure-revealjs/unnamed-chunk-9-1.png){width=960}\n:::\n:::\n\n:::\n::::\n\n## Transformations: Asymptotic relationship\n\n::::{.columns}\n:::{.column width=\"50%\"}\n#### Before transformation\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(data = asymptotic,\n       aes(x = predictor, y = response)) +\n  geom_point() +\n  labs(x = \"Predictor\", y = \"Response\")\n```\n\n::: {.cell-output-display}\n![](envx1001-w12-lecture_files/figure-revealjs/unnamed-chunk-10-1.png){width=960}\n:::\n:::\n\n:::\n:::{.column width=\"50%\"}\n:::{.fragment}\n#### After log~e~ transform\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(data = asymptotic, \n       aes(x = log(predictor), y = response)) +\n  geom_point() +\n  labs(x = \"Predictor\", y = \"Response\")\n```\n\n::: {.cell-output-display}\n![](envx1001-w12-lecture_files/figure-revealjs/unnamed-chunk-11-1.png){width=960}\n:::\n:::\n\n:::\n:::\n::::\n\n## Transformations: Asymptotic relationship\n\n::::{.columns}\n:::{.column width=\"50%\"}\n#### Before transformation\n\n::: {.cell}\n\n```{.r .cell-code}\nautoplot(lm(response ~ predictor, data = asymptotic)) +\n  labs(x = \"Predictor\", y = \"Response\")\n```\n\n::: {.cell-output-display}\n![](envx1001-w12-lecture_files/figure-revealjs/unnamed-chunk-12-1.png){width=960}\n:::\n:::\n\n:::\n:::{.column width=\"50%\"}\n#### After log~e~ transform\n\n::: {.cell}\n\n```{.r .cell-code}\nautoplot(lm(log(response) ~ predictor, data = asymptotic)) +\n  labs(x = \"Predictor\", y = \"Response\")\n```\n\n::: {.cell-output-display}\n![](envx1001-w12-lecture_files/figure-revealjs/unnamed-chunk-13-1.png){width=960}\n:::\n:::\n\n:::\n::::\n\n## Transformations: Logistic relationship\n\n::::{.columns}\n:::{.column width=\"50%\"}\n#### Before transformation\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(data = logistic,\n       aes(x = predictor, y = response)) +\n  geom_point() +\n  labs(x = \"Predictor\", y = \"Response\")\n```\n\n::: {.cell-output-display}\n![](envx1001-w12-lecture_files/figure-revealjs/unnamed-chunk-14-1.png){width=960}\n:::\n:::\n\n:::\n:::{.column width=\"50%\"}\n:::{.fragment}\n#### After log~e~ transform\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(data = logistic, \n       aes(x = predictor, y = log(response))) +\n  geom_point() +\n  labs(x = \"Predictor\", y = \"Response\")\n```\n\n::: {.cell-output-display}\n![](envx1001-w12-lecture_files/figure-revealjs/unnamed-chunk-15-1.png){width=960}\n:::\n:::\n\n:::\n:::\n::::\n\n## Transformations: Logistic relationship\n\n::::{.columns}\n:::{.column width=\"50%\"}\n#### Before transformation\n\n::: {.cell}\n\n```{.r .cell-code}\nautoplot(lm(response ~ predictor, data = logistic)) +\n  labs(x = \"Predictor\", y = \"Response\")\n```\n\n::: {.cell-output-display}\n![](envx1001-w12-lecture_files/figure-revealjs/unnamed-chunk-16-1.png){width=960}\n:::\n:::\n\n:::\n:::{.column width=\"50%\"}\n#### After log~e~ transform\n\n::: {.cell}\n\n```{.r .cell-code}\nautoplot(lm(log(response) ~ predictor, data = logistic)) +\n  labs(x = \"Predictor\", y = \"Response\")\n```\n\n::: {.cell-output-display}\n![](envx1001-w12-lecture_files/figure-revealjs/unnamed-chunk-17-1.png){width=960}\n:::\n:::\n\n:::\n::::\n\n\n## Transformations: Curvilinear relationship\n\n::::{.columns}\n:::{.column width=\"50%\"}\n#### Before transformation\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(data = curvilinear,\n       aes(x = predictor, y = response)) +\n  geom_point() +\n  labs(x = \"Predictor\", y = \"Response\")\n```\n\n::: {.cell-output-display}\n![](envx1001-w12-lecture_files/figure-revealjs/unnamed-chunk-18-1.png){width=960}\n:::\n:::\n\n:::\n:::{.column width=\"50%\"}\n:::{.fragment}\n#### After log~e~ transform\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(data = curvilinear, \n       aes(x = predictor, y = log(response))) +\n  geom_point() +\n  labs(x = \"Predictor\", y = \"Response\")\n```\n\n::: {.cell-output-display}\n![](envx1001-w12-lecture_files/figure-revealjs/unnamed-chunk-19-1.png){width=960}\n:::\n:::\n\n:::\n:::\n::::\n\n## Transformations: Curvilinear relationship\n\n::::{.columns}\n:::{.column width=\"50%\"}\n#### Before transformation\n\n::: {.cell}\n\n```{.r .cell-code}\nautoplot(lm(response ~ predictor, data = curvilinear)) +\n  labs(x = \"Predictor\", y = \"Response\")\n```\n\n::: {.cell-output-display}\n![](envx1001-w12-lecture_files/figure-revealjs/unnamed-chunk-20-1.png){width=960}\n:::\n:::\n\n:::\n:::{.column width=\"50%\"}\n#### After log~e~ transform\n\n::: {.cell}\n\n```{.r .cell-code}\nautoplot(lm(log(response) ~ predictor, data = curvilinear)) +\n  labs(x = \"Predictor\", y = \"Response\")\n```\n\n::: {.cell-output-display}\n![](envx1001-w12-lecture_files/figure-revealjs/unnamed-chunk-21-1.png){width=960}\n:::\n:::\n\n:::\n::::\n\n\n## Did the transformations work?\n\n- To a *certain* extent...\n- **Problems**:\n  - Relationships typically do not meet the linear assumption, but seem \"ok\" for other assumptions.\n  - Poor fit to the data (over or underfitting in some areas).\n  - Difficult to interpret the results.\n\n\n## Nonlinear regression\n\n- A way to model complex (nonlinear) relationships.\n  - i.e. phenomena that arise in the natural and physical sciences e.g. biology, chemistry, physics, engineering.\n- At least *one* predictor is not linearly related to the response variable.\n\n\n# Performing nonlinear regression\n> \"You need to know a bit of *calculus*.\"\n\n. . .\n\n![](images/nope.gif)\n\n\n## Wait!\n\n. . .\n\n- It's easier than you think in R.\n- **Polynomial regression**: still linear in the parameters and a good place to start.\n- **Nonlinear regression**: use the `nls()` function to fit the following nonlinear models:\n  - Exponential growth\n  - Exponential decay\n  - Logistic\n\n\n# Polynomial regression\n> A special case of multiple linear regression used to model nonlinear relationships.\n\n## Model\n\n$$ Y_i = \\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + ... + \\beta_k x_i^k + \\epsilon_i $$\n\nwhere $k$ is the degree of the polynomial.\n\n- The model is still linear in the parameters $\\beta$ and can be fitted using least squares.\n- Instead of multiple predictors, we have multiple *terms* of the same predictor.\n- Can still be fit using `lm()`.\n\n. . .\n\n<br>\n\n### Adding polynomial terms\n- Linear: $y = \\beta_0 + \\beta_1 x$\n- Quadratic: $y = \\beta_0 + \\beta_1 x + \\beta_2 x^2$\n- Cubic: $y = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\beta_3 x^3$\n- Each level increases the power of the predictor by 1.\n\n# Polynomial fitting\nUsing the `asymptotic` data\n\n## The data\n\nSee Slide 11 for the relationship and mathematical expression.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(asymptotic, aes(x = predictor, y = response)) +\n  geom_point() +\n  labs(x = \"Predictor\", y = \"Response\")\n```\n\n::: {.cell-output-display}\n![](envx1001-w12-lecture_files/figure-revealjs/unnamed-chunk-22-1.png){width=960}\n:::\n:::\n\n\n## Fitting the model (linear)\n\n$$ Y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i $$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlin_fit <- lm(response ~ predictor, asymptotic)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nggplot(asymptotic, aes(x = predictor, y = response)) +\n  geom_point() +\n  labs(x = \"Predictor\", y = \"Response\") +\n  geom_line(aes(y = predict(lin_fit)), color = \"red\", size = 2)\n```\n\n::: {.cell-output-display}\n![](envx1001-w12-lecture_files/figure-revealjs/unnamed-chunk-24-1.png){width=960}\n:::\n:::\n\n\n## Fitting the model (poly order 2)\n\n$$ Y_i = \\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + \\epsilon_i $$\n\n\n::: {.cell}\n\n```{.r .cell-code}\npoly2_fit <- lm(response ~ poly(predictor, 2), asymptotic)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nggplot(asymptotic, aes(x = predictor, y = response)) +\n  geom_point() +\n  labs(x = \"Predictor\", y = \"Response\") +\n  geom_line(aes(y = predict(lin_fit)), color = \"red\") +\n  geom_line(aes(y = predict(poly2_fit)), color = \"slateblue\", size = 2)\n```\n\n::: {.cell-output-display}\n![](envx1001-w12-lecture_files/figure-revealjs/unnamed-chunk-26-1.png){width=960}\n:::\n:::\n\n\n## Fitting the model (poly order 3)\n\n$$ Y_i = \\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + \\beta_3 x_i^3 + \\epsilon_i $$\n\n\n::: {.cell}\n\n```{.r .cell-code}\npoly3_fit <- lm(response ~ poly(predictor, 3), asymptotic)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nggplot(asymptotic, aes(x = predictor, y = response)) +\n  geom_point() +\n  labs(x = \"Predictor\", y = \"Response\") +\n  geom_line(aes(y = predict(lin_fit)), color = \"red\") +\n  geom_line(aes(y = predict(poly2_fit)), color = \"slateblue\") +\n  geom_line(aes(y = predict(poly3_fit)), color = \"seagreen\", size = 2)\n```\n\n::: {.cell-output-display}\n![](envx1001-w12-lecture_files/figure-revealjs/unnamed-chunk-28-1.png){width=960}\n:::\n:::\n\n\n## Fitting the model (poly order 10)\n\n$$ Y_i = \\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + ... + \\beta_10 x_i^{10} + \\epsilon_i $$\n\n\n::: {.cell}\n\n```{.r .cell-code}\npoly10_fit <- lm(response ~ poly(predictor, 10), asymptotic)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nggplot(asymptotic, aes(x = predictor, y = response)) +\n  geom_point() +\n  labs(x = \"Predictor\", y = \"Response\") +\n  geom_line(aes(y = predict(lin_fit)), color = \"red\") +\n  geom_line(aes(y = predict(poly2_fit)), color = \"slateblue\") +\n  geom_line(aes(y = predict(poly3_fit)), color = \"seagreen\") +\n  geom_line(aes(y = predict(poly10_fit)), color = \"firebrick\", size = 2)\n```\n\n::: {.cell-output-display}\n![](envx1001-w12-lecture_files/figure-revealjs/unnamed-chunk-30-1.png){width=960}\n:::\n:::\n\n\n\n## Limitations\n\n- Meaning of the coefficients is not always clear.\n- Extrapolation can be *dangerous*.\n- Extra terms can lead to overfitting and are difficult to interpret:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(poly10_fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = response ~ poly(predictor, 10), data = asymptotic)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-17.1659  -8.6908  -0.0494   8.8003  16.4012 \n\nCoefficients:\n                      Estimate Std. Error t value Pr(>|t|)    \n(Intercept)             79.818      1.552  51.426  < 2e-16 ***\npoly(predictor, 10)1   159.368     11.084  14.378  < 2e-16 ***\npoly(predictor, 10)2  -106.939     11.084  -9.648 5.37e-12 ***\npoly(predictor, 10)3    48.570     11.084   4.382 8.28e-05 ***\npoly(predictor, 10)4   -19.411     11.084  -1.751   0.0876 .  \npoly(predictor, 10)5     1.193     11.084   0.108   0.9148    \npoly(predictor, 10)6    -2.769     11.084  -0.250   0.8040    \npoly(predictor, 10)7    -1.343     11.084  -0.121   0.9042    \npoly(predictor, 10)8    -4.009     11.084  -0.362   0.7195    \npoly(predictor, 10)9    -2.851     11.084  -0.257   0.7984    \npoly(predictor, 10)10    5.769     11.084   0.520   0.6056    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11.08 on 40 degrees of freedom\nMultiple R-squared:  0.8897,\tAdjusted R-squared:  0.8621 \nF-statistic: 32.26 on 10 and 40 DF,  p-value: 4.846e-16\n```\n\n\n:::\n:::\n\n\n\n### But:\n\n- Easy to fit: just add polynomial terms to the model.\n- Simple to perform: use `lm()`.\n\n\n# Nonlinear fitting\n\n## Fitting a nonlinear model\n\nIf you have some understanding of the underlying relationship (e.g. mechanistic process) between the variables, you can fit a nonlinear model.\n<br>\n\n:::{.fragment}\n\n### Mathematical expression\n\n$$ Y_i = f(x_i, \\beta) + \\epsilon_i $$\n\nwhere $f(x_i, \\beta)$ is a nonlinear function of the parameters $\\beta$.\n\n- $Y_i$ is the continuous response variable.\n- $x_i$ is the vector of predictor variables.\n- $\\beta$ is the vector of unknown parameters.\n- $\\epsilon_i$ is the random error term (residual error).\n:::\n\n## Assumptions\n\nLike the linear model, the nonlinear model assumes:\n\n- Error terms are normally distributed (**Normality**).\n- Error terms are independent (**Independence**).\n- Error terms have constant variance (**Homoscedasticity**).\n\nBasically:\n\n$$ \\epsilon_i \\sim N(0, \\sigma^2) $$\n\n. . .\n\nLike all other models we have seen, we focus on the residuals to assess the model fit, since the residuals are the only part of the model that is random.\n\n\n## Estimating the model parameters\n\n- The parameters are estimated using the **method of least squares**.\n- For nonlinear models, a nonlinear optimization algorithm is used to find the best fit, rather than ordinary least squares, e.g.:\n  - [Gauss-Newton algorithm](https://en.wikipedia.org/wiki/Gauss%E2%80%93Newton_algorithm)\n  - [Levenberg-Marquardt algorithm](https://en.wikipedia.org/wiki/Levenberg%E2%80%93Marquardt_algorithm)\n- This can only be performed iteratively and depends on a \"best guess\" of the parameters *as a start*.\n  - **i.e. we need to provide a starting point for a nonlinear least squares algorithm to begin**.\n\n---\n\n![](images/gauss-newton.gif)\n\nSource: [Wikipedia](https://en.wikipedia.org/wiki/Gauss%E2%80%93Newton_algorithm)\n\n## Implementation\n\nuse `nls()` function in R.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnls(formula, data, start)\n```\n:::\n\n\n- `formula`: a formula object, with the response variable on the left of a ~ operator, and the predictor variable(s) on the right.\n- `data`: a data frame containing the variables in the model.\n- `start`: a named list of starting values for the parameters in the model.\n\n\n# Example: Fitting an asymptotic model\n\n## Finding starting values\n\n$$ y = a + b(1 - e^{-cx}) $$\n\n- $a$ is value of $y$ when $x = 0$.\n- $b$ is the upper limit: the maximum value of $y$.\n- $c$ is the rate of change: the rate at which $y$ approaches the upper limit.\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nggplot(data = asymptotic, aes(x = predictor, y = response)) +\n  geom_point() + \n  geom_hline(yintercept = 100, linetype = \"dashed\") +\n  geom_vline(xintercept = 0, linetype = \"dashed\") +\n  ## plot the rate\n  geom_segment(aes(x = 0, y = 0, xend = 2.5, yend = 100), \n               arrow = arrow(length = unit(0.5, \"cm\")), \n               color = \"red\") +\n  labs(x = \"Predictor\", y = \"Response\")\n```\n\n::: {.cell-output-display}\n![](envx1001-w12-lecture_files/figure-revealjs/unnamed-chunk-33-1.png){width=960}\n:::\n:::\n\n\n## First guess\n\n$$ y = a + b(1 - e^{-cx}) $$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_asymptotic <- nls(response ~ a + b*(1-exp(-c*predictor)), data = asymptotic, \n  start = list(a = 0, b = 100, c = 0.8))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nggplot(data = asymptotic, aes(x = predictor, y = response)) +\n  geom_point() + \n  geom_hline(yintercept = 100, linetype = \"dashed\") +\n  geom_vline(xintercept = 0, linetype = \"dashed\") +\n  ## plot the rate\n  geom_segment(aes(x = 0, y = 0, xend = 0, yend = 100), \n               arrow = arrow(length = unit(0.5, \"cm\")), \n               color = \"red\") +\n  labs(x = \"Predictor\", y = \"Response\") +\n  geom_line(aes(y = predict(fit_asymptotic)), color = \"red\", size = 2)\n```\n\n::: {.cell-output-display}\n![](envx1001-w12-lecture_files/figure-revealjs/unnamed-chunk-35-1.png){width=960}\n:::\n:::\n\n\n## Check the fit\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(nlstools)\nresids <- nlsResiduals(fit_asymptotic)\nplot(resids)\n```\n\n::: {.cell-output-display}\n![](envx1001-w12-lecture_files/figure-revealjs/unnamed-chunk-36-1.png){width=960}\n:::\n:::\n\n\n## Interpretation\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(fit_asymptotic)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nFormula: response ~ a + b * (1 - exp(-c * predictor))\n\nParameters:\n   Estimate Std. Error t value Pr(>|t|)    \na -14.51755    6.64162  -2.186   0.0337 *  \nb 113.03797    6.44716  17.533  < 2e-16 ***\nc   0.62962    0.07142   8.816 1.32e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 10.21 on 48 degrees of freedom\n\nNumber of iterations to convergence: 4 \nAchieved convergence tolerance: 1.178e-06\n```\n\n\n:::\n:::\n\n\n- The model is significant since the p-value is less than 0.05 for all parameters.\n- The parameterised model is:\n\n$$ y = -14.5 + 113.04(1 - e^{-0.63x}) $$\n\n:::{.callout-note}\nThe R-square value is not reported for nonlinear models as the sum of squares is not partitioned into explained and unexplained components.\n:::\n\n\n# Another example: Fitting a logistic model\n\n## Recap on logistic relationship\n\n$$ y = c + \\frac{d-c}{1+e^{-b(x-a)}} $$\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nggplot(data = logistic, aes(x = predictor, y = response)) +\n  geom_point() +\n  labs(x = \"Predictor\", y = \"Response\")\n```\n\n::: {.cell-output-display}\n![](envx1001-w12-lecture_files/figure-revealjs/unnamed-chunk-38-1.png){width=960}\n:::\n:::\n\n\n## Recap on logistic relationship\n\n$$ y = c + \\frac{d-c}{1+e^{-b(x-a)}} $$\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nggplot(data = logistic, aes(x = predictor, y = response)) +\n  geom_point() +\n  geom_smooth() +\n  labs(x = \"Predictor\", y = \"Response\")\n```\n\n::: {.cell-output-display}\n![](envx1001-w12-lecture_files/figure-revealjs/unnamed-chunk-39-1.png){width=960}\n:::\n:::\n\n\n\n## Finding the starting values\n\n$$ y = c + \\frac{d-c}{1+e^{-b(x-a)}} $$\n\n- $c$ is the lower limit: the minimum value of $y$.\n- $d$ is the upper limit: the maximum value of $y$.\n- $a$ is the value of $x$ when $y$ is halfway between the lower and upper limits.\n- $b$ is the rate of change: the rate at which $y$ approaches the upper limit.\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nggplot(data = logistic, aes(x = predictor, y = response)) +\n  geom_point() +\n  geom_smooth() +\n  labs(x = \"Predictor\", y = \"Response\") +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  geom_hline(yintercept = 300, linetype = \"dashed\") +\n  geom_vline(xintercept = 5, linetype = \"dashed\") +\n  # label the lines above\n  annotate(\"text\", x = 9, y = 0, label = \"c\", size = 8, vjust = -1) +\n  annotate(\"text\", x = 0, y = 300, label = \"d\", size = 8, vjust = 1.5) +\n  annotate(\"text\", x = 5, y = 100, label = \"a\", size = 8, hjust = -1) +\n  ## plot the rate\n  geom_segment(aes(x = 2.5, y = 60, xend = 6, yend = 250), \n               arrow = arrow(length = unit(0.5, \"cm\")), \n               color = \"red\") +\n  # label the rate\n  annotate(\"text\", x = 4, y = 180, label = \"b\", size = 8, colour = \"red\", hjust = -1)\n```\n\n::: {.cell-output-display}\n![](envx1001-w12-lecture_files/figure-revealjs/unnamed-chunk-40-1.png){width=960}\n:::\n:::\n\n\n\n## Finding the starting values\n\n$$ y = c + \\frac{d-c}{1+e^{-b(x-a)}} $$\n\n- $c$ is the lower limit: the minimum value of $y$.\n- $d$ is the upper limit: the maximum value of $y$.\n- $a$ is the value of $x$ when $y$ is halfway between the lower and upper limits.\n- $b$ is the rate of change: the rate at which $y$ approaches the upper limit.\n\n![](images/nope2.gif){fig-align=\"center\"}\n\n<center> **NOPE** </center>\n\n# Automating the process (sort of)\n\n## Self-starting functions\n\n- The `nls()` function requires a formula and starting point(s) for the parameters.\n  - *How about starting to nope out...*\n\n:::{.fragment}\n### Wait!\n\n- Several self-starting functions are available in R that can be used to estimate the starting values.\n- These functions are named after the model they fit, e.g. `SSasymp()`, `SSlogis()`, `SSmicmen()`, `SSweibull()`, etc.\n\n:::\n\n:::{.fragment}\n:::{.callout-important}\nWe still need to have some understanding of the underlying relationship between the variables to pick the right function.\n:::\n:::\n\n## Revisiting the logistic model\n\n$$ y = c + \\frac{d-c}{1+e^{-b(x-a)}} $$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nSSlogis(input, Asym, xmid, scal)\n```\n:::\n\n\n- `input`: the predictor variable.\n- `Asym`: the upper limit.\n- `xmid`: the value of $x$ when $y$ is halfway between the lower and upper limits.\n- `scal`: the rate of change.\n\nThe equation ia *different*: see `?SSlogis`:\n\n$$ y = \\frac{Asym}{1+exp \\frac{xmid-input}{scal}} $$\n\n. . .\n\nOther than `input`, the other parameters can be left to the function to estimate.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_logistic <- nls(response ~ SSlogis(predictor, Asym, xmid, scal), data = logistic)\n```\n:::\n\n\n## What does the fit look like?\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nggplot(data = logistic, aes(x = predictor, y = response)) +\n  geom_point() +\n  labs(x = \"Predictor\", y = \"Response\") +\n  geom_line(aes(y = predict(fit_logistic)), color = \"red\", size = 1)\n```\n\n::: {.cell-output-display}\n![](envx1001-w12-lecture_files/figure-revealjs/unnamed-chunk-43-1.png){width=960}\n:::\n:::\n\n\n## Check the fit\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresids <- nlsResiduals(fit_logistic)\nplot(resids)\n```\n\n::: {.cell-output-display}\n![](envx1001-w12-lecture_files/figure-revealjs/unnamed-chunk-44-1.png){width=960}\n:::\n:::\n\n\n## Interpretation\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(fit_logistic)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nFormula: response ~ SSlogis(predictor, Asym, xmid, scal)\n\nParameters:\n      Estimate Std. Error t value Pr(>|t|)    \nAsym 310.64727    4.62579   67.16   <2e-16 ***\nxmid   4.92715    0.07142   68.99   <2e-16 ***\nscal   1.34877    0.05418   24.90   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 10.22 on 48 degrees of freedom\n\nNumber of iterations to convergence: 1 \nAchieved convergence tolerance: 6.626e-07\n```\n\n\n:::\n:::\n\n\n\n# Back to asymptotic model\n\n## Comparing manual fit to self-starting function\n\nThe self-starting function for the asymptotic model is `SSasymp()`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_asymptotic_ss <- nls(response ~ SSasymp(predictor, Asym, R0, lrc), data = asymptotic)\n```\n:::\n\n\nComparing outputs:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(data = asymptotic, aes(x = predictor, y = response)) +\n  geom_point() +\n  labs(x = \"Predictor\", y = \"Response\") +\n  geom_line(aes(y = predict(fit_asymptotic)), color = \"red\", size = 1) +\n  geom_line(aes(y = predict(fit_asymptotic_ss)), color = \"blue\", size = 1)\n```\n\n::: {.cell-output-display}\n![](envx1001-w12-lecture_files/figure-revealjs/unnamed-chunk-47-1.png){width=960}\n:::\n:::\n\n\nIn some cases, the fits are identical, but in others, they are not.\n\n## Summary\n\n- When fitting a nonlinear model, there are three possible approaches:\n   1. **Linearize** the model by transforming the response variable or predictor variable(s): \n      - Fit: easy/difficult\n      - Interpret: difficult\n   2. Approximate the model by adding **polynomial** terms: \n      - Fit: easy\n      - Interpret: difficult\n   3. Fit the model using a **nonlinear** least squares algorithm:\n      - Fit: difficult\n      - Interpret: easy\n  \n. . .\n\n- Nonlinear models:\n  - Useful for modelling complex relationships.\n  - Require some understanding of the underlying relationship between the variables, especially asympotic and logistic models.\n  - Most useful when prediction is the goal, since we do not necessarily need to interpret the parameters to assess the model fit.\n\n\n# Bonus: How do we know which model is better?\nNote: this is non-examinable content but might be useful for your project.\n\n## Example: polynomial regression\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(tidyr)\n\n# Create a new data frame with predictor values and model predictions\npredictions <- data.frame(\n  predictor = asymptotic$predictor,\n  Linear = predict(lin_fit),\n  Poly_2 = predict(poly2_fit),\n  Poly_3 = predict(poly3_fit),\n  Poly_10 = predict(poly10_fit)\n)\n\n# Reshape the data to long format\npredictions_long <- predictions %>%\n  pivot_longer(cols = -predictor, names_to = \"Model\", values_to = \"response\")\n\n# Plot the data\nggplot(predictions_long, aes(x = predictor, y = response, color = Model)) +\n  geom_point(data = asymptotic, aes(x = predictor, y = response), inherit.aes = FALSE) +\n  geom_line(linewidth = 1) +\n  labs(x = \"Predictor\", y = \"Response\") +\n  scale_color_brewer(palette = \"Spectral\") \n```\n\n::: {.cell-output-display}\n![](envx1001-w12-lecture_files/figure-revealjs/unnamed-chunk-48-1.png){width=960}\n:::\n:::\n\n\n## Prediction quality\n\nWe can use prediction quality metrics to compare the fits.\n\n- [Akaike information criterion (AIC)](https://en.wikipedia.org/wiki/Akaike_information_criterion) and\n[Bayesian information criterion (BIC)](https://en.wikipedia.org/wiki/Bayesian_information_criterion).\n  - Useful for comparing model fits.\n- [root mean squared error (RMSE)](https://en.wikipedia.org/wiki/Root-mean-square_deviation)\nand [mean absolute error (MAE)](https://en.wikipedia.org/wiki/Mean_absolute_error).\n  - Useful for *assessing the quality* of the fit.\n\n\n\n## AIC and BIC\n\nUse the `broom` package to extract the AIC and BIC values from the model fits.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(broom)\n# collect all polynomial fits into a single tibble using glance\npoly_fits <- tibble(\n  model = c(\"linear\", \"poly2\", \"poly3\", \"poly10\"),\n  fit = list(lin_fit, poly2_fit, poly3_fit, poly10_fit)) %>%\n  mutate(glance = map(fit, glance)) %>%\n  unnest(glance) %>%\n  select(model, AIC, BIC)\npoly_fits\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4 × 3\n  model    AIC   BIC\n  <chr>  <dbl> <dbl>\n1 linear  453.  459.\n2 poly2   409.  416.\n3 poly3   392.  402.\n4 poly10  402.  425.\n```\n\n\n:::\n:::\n\n\n- The smaller the AIC or BIC, the better the fit compared to other models.\n- However, for better performance, **cross-validation is recommended** as it explains how well the model will perform on **new** data, rather than just assessing the fit to the data.\n\n## Cross-validation\n\n1. Split the data into training and testing sets.\n2. Fit the model to the training set.\n3. Predict the response variable using the testing set.\n4. Calculate the RMSE or MAE between the predicted and observed values.\n5. Repeat for each fold of the data.\n6. Average the RMSE or MAE across all folds.\n7. The model with the lowest RMSE or MAE is the best model.\n\nLooks like a lot of work, but it's easy in R using the `caret` package.\n\n## Performing cross-validation on the polynomial fits\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(caret)\nctrl <- trainControl(method = \"cv\", number = 10) # Set up control for 10-fold cross-validation\n# Fit models for degrees 1, 2, 3, and 10\nlin_fit <- train(response ~ predictor, data = asymptotic, method = \"lm\", trControl = ctrl)\npoly2_fit <- train(response ~ poly(predictor, 2), data = asymptotic, method = \"lm\", trControl = ctrl)\npoly3_fit <- train(response ~ poly(predictor, 3), data = asymptotic, method = \"lm\", trControl = ctrl)\npoly10_fit <- train(response ~ poly(predictor, 10), data = asymptotic, method = \"lm\", trControl = ctrl)\n\nresults <- resamples(list(linear = lin_fit, quadratic = poly2_fit, cubic = poly3_fit, poly10 = poly10_fit))\n\n# Extract mean RMSE and MAE\nMAE <- summary(results)$statistics$MAE[, 4] # select mean column only\nRMSE <- summary(results)$statistics$RMSE[, 4] # select mean column only\nknitr::kable(data.frame(MAE, RMSE))\n```\n\n::: {.cell-output-display}\n\n\n|          |       MAE|     RMSE|\n|:---------|---------:|--------:|\n|linear    | 15.751679| 19.76680|\n|quadratic |  9.881451| 11.93538|\n|cubic     |  9.741090| 11.27664|\n|poly10    | 11.774449| 13.70916|\n\n\n:::\n:::\n\n\nFrom the results, the cubic model has the lowest RMSE and MAE, so is the best model.\n\n\n\n# Thanks!\n\nThis presentation is based on the [SOLES Quarto reveal.js template](https://github.com/usyd-soles-edu/soles-revealjs) and is licensed under a [Creative Commons Attribution 4.0 International License][cc-by].\n\n\n<!-- Links -->\n[cc-by]: http://creativecommons.org/licenses/by/4.0/",
    "supporting": [
      "envx1001-w12-lecture_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}